{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 18 - Transformer\n",
    "\n",
    " This Notebook covered how to train and test a Transformer model using the IWSLT2016 dataset. We discussed preparing the dataset, setting up the training loop with appropriate loss and optimization techniques, and evaluating the model's performance on unseen data. This hands-on approach provides a practical understanding of training and testing Transformer models for language translation tasks. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c60c23f5580006f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da82416457f72d7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torchtext.datasets import IWSLT2016\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Tokenizer\n",
    "de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "# Load dataset\n",
    "train_iter, _, _ = IWSLT2016(split='train', language_pair=('de', 'en'))\n",
    "\n",
    "# Build vocab\n",
    "de_vocab = build_vocab_from_iterator((de_tokenizer(de) for de, _ in train_iter),\n",
    "                                     specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "en_vocab = build_vocab_from_iterator((en_tokenizer(en) for _, en in train_iter),\n",
    "                                     specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "de_vocab.set_default_index(de_vocab[\"<unk>\"])\n",
    "en_vocab.set_default_index(en_vocab[\"<unk>\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4a2deb2fbe8bcb9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def data_process(raw_data_iter): \n",
    "    data = [] \n",
    "    for (raw_de, raw_en) in raw_data_iter: \n",
    "        de_tensor = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long) \n",
    "        en_tensor = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long) \n",
    "        data.append((de_tensor, en_tensor)) \n",
    "    return data "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T02:45:46.922823Z",
     "start_time": "2024-03-04T02:45:46.919677Z"
    }
   },
   "id": "2b51d2ce36c71d81",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# DataLoader \n",
    "\n",
    "def collate_fn(batch): \n",
    "    de_batch, en_batch = [], [] \n",
    "    for de_item, en_item in batch: \n",
    "        de_batch.append(torch.cat([torch.tensor([de_vocab[\"<bos>\"]]), de_item, torch.tensor([de_vocab[\"<eos>\"]])], dim=0)) \n",
    "        en_batch.append(torch.cat([torch.tensor([en_vocab[\"<bos>\"]]), en_item, torch.tensor([en_vocab[\"<eos>\"]])], dim=0)) \n",
    "\n",
    "    de_batch = pad_sequence(de_batch, padding_value=de_vocab[\"<pad>\"]) \n",
    "    en_batch = pad_sequence(en_batch, padding_value=en_vocab[\"<pad>\"]) \n",
    "\n",
    "    return de_batch, en_batch "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T02:46:10.266297Z",
     "start_time": "2024-03-04T02:46:10.261243Z"
    }
   },
   "id": "9945ca30377ba0d2",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = data_process(train_iter) \n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50c3f254984478b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "208fc0a28f448685"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "import torch.nn as nn \n",
    "from transformer import Transformer\n",
    "\n",
    "# Model, Loss, and Optimizer \n",
    "model = Transformer(embed_size=512, num_layers=6, heads=8, ff_hidden_size=2048, dropout_rate=0.1, vocab_size=len(en_vocab), max_length=100) \n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=en_vocab[\"<pad>\"]) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001) \n",
    "\n",
    "# Training loop \n",
    "num_epochs = 10 \n",
    "for epoch \n",
    "    model.train() \n",
    "    total_loss = 0 \n",
    "    for de_batch, en_batch in train_dataloader: \n",
    "        optimizer.zero_grad() \n",
    "        output = model(de_batch, en_batch[:-1])  # Exclude <eos> token for target input \n",
    "        loss = loss_fn(output.reshape(-1, output.size(-1)), en_batch[1:].reshape(-1))  # Shift target for loss calculation \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        total_loss += loss.item() \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\") "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d29681969473a3a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Eval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6303d485d60a37a6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu \n",
    "\n",
    "def evaluate(model, data_iter): \n",
    "    model.eval() \n",
    "    predictions = [] \n",
    "    references = [] \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for de_batch, en_batch in data_iter: \n",
    "            output = model(de_batch)  # No target input for evaluation \n",
    "            output = output.argmax(dim=-1) \n",
    "            predictions.extend([[en_vocab.itos[idx] for idx in sentence] for sentence in output]) \n",
    "            references.extend([[[en_vocab.itos[idx] for idx in sentence]] for sentence in en_batch]) \n",
    "    bleu_score = corpus_bleu(references, predictions) \n",
    "    print(f\"BLEU Score: {bleu_score}\") \n",
    "\n",
    "# Assuming test_data is prepared similar to train_data \n",
    "test_iter, _ = IWSLT2016(split='test', language_pair=('de', 'en')) \n",
    "test_data = data_process(test_iter) \n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collate_fn) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67600e10c1f9ec3d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "evaluate(model, test_dataloader) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4875eb167b16ba7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
